{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f86087-7984-47cb-bc78-794c4a994777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchvision import transforms\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca767615-1fb7-47bf-9432-02e3c52dd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletPaperDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_root, processor, image_size=224):\n",
    "        \"\"\"\n",
    "        csv_path: path to your CSV with columns [paper_id, question, positive, negative]\n",
    "        image_root: root folder for 'train/' \n",
    "        processor: a HuggingFace CLIPProcessor\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_root = image_root\n",
    "        self.processor = processor\n",
    "        # fallback transforms (if not using processor for images)\n",
    "        self.default_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=self.processor.feature_extractor.image_mean,\n",
    "                std=self.processor.feature_extractor.image_std\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        question = row['question']\n",
    "\n",
    "        # load images\n",
    "        pos_path = os.path.join(self.image_root, row['positive'])\n",
    "        neg_path = os.path.join(self.image_root, row['negative'])\n",
    "        img_pos = Image.open(pos_path).convert('RGB')\n",
    "        img_neg = Image.open(neg_path).convert('RGB')\n",
    "\n",
    "        # use processor to tokenize text and preprocess images\n",
    "        encoding = self.processor(\n",
    "            text=[question, question],\n",
    "            images=[img_pos, img_neg],\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,                             # enforce at most 77 tokens for CLIP\n",
    "            max_length=self.processor.tokenizer.model_max_length\n",
    "        )\n",
    "\n",
    "        # encoding fields: input_ids, attention_mask, pixel_values\n",
    "        return {\n",
    "            'question':      question,\n",
    "            'input_ids':      encoding['input_ids'][0],\n",
    "            'attention_mask': encoding['attention_mask'][0],\n",
    "            'img_pos':        img_pos,\n",
    "            'img_neg':        img_neg\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93e2a1f8-ea90-4fd4-990b-a9e32fef847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip(\n",
    "    csv_path,\n",
    "    image_root='',\n",
    "    model_name='openai/clip-vit-base-patch32',\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    lr=5e-6,\n",
    "    margin=0.2,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "):\n",
    "    # 1. Prepare model & processor\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # 2. Dataset & DataLoader\n",
    "\n",
    "    dataset = TripletPaperDataset(csv_path, image_root, processor)\n",
    "    def collate_fn(batch):\n",
    "        questions = [b[\"question\"] for b in batch]\n",
    "        images     = sum([[b[\"img_pos\"], b[\"img_neg\"]] for b in batch], [])\n",
    "    \n",
    "        batch_enc = processor(\n",
    "            text=questions,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=processor.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        pv = batch_enc.pixel_values.view(len(batch), 2, *batch_enc.pixel_values.shape[1:])\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\":      batch_enc.input_ids,\n",
    "            \"attention_mask\": batch_enc.attention_mask,\n",
    "            \"img_pos\":        pv[:,0],\n",
    "            \"img_neg\":        pv[:,1],\n",
    "        }\n",
    "\n",
    "    loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=0,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "\n",
    "    # 3. Loss & optimizer\n",
    "    criterion = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss = 0.0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # move inputs to device\n",
    "            input_ids      = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            img_pos        = batch['img_pos'].to(device)\n",
    "            img_neg        = batch['img_neg'].to(device)\n",
    "\n",
    "            # 4. Forward passes\n",
    "            text_outputs    = model.get_text_features(input_ids=input_ids,\n",
    "                                                      attention_mask=attention_mask)\n",
    "            pos_img_outputs = model.get_image_features(pixel_values=img_pos)\n",
    "            neg_img_outputs = model.get_image_features(pixel_values=img_neg)\n",
    "\n",
    "            # 5. L2-normalize embeddings\n",
    "            text_emb = text_outputs / text_outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "            pos_emb  = pos_img_outputs / pos_img_outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "            neg_emb  = neg_img_outputs / neg_img_outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "            # 6. Compute Triplet Loss\n",
    "            loss = criterion(text_emb, pos_emb, neg_emb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch {epoch}/{epochs} — avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 7. Save the fine-tuned model\n",
    "    save_dir = \"clip_finetuned\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.save_pretrained(save_dir)\n",
    "    processor.save_pretrained(save_dir)\n",
    "    print(f\"Model saved to {save_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "953a5ef9-84d5-4941-9967-6b3c84738bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudhjoshi/miniforge3/envs/14-763/lib/python3.12/site-packages/transformers/models/clip/processing_clip.py:149: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 — avg loss: 0.2240\n",
      "Epoch 2/5 — avg loss: 0.1688\n",
      "Epoch 3/5 — avg loss: 0.1130\n",
      "Epoch 4/5 — avg loss: 0.0674\n",
      "Epoch 5/5 — avg loss: 0.0385\n",
      "Model saved to clip_finetuned/\n"
     ]
    }
   ],
   "source": [
    "train_clip(\n",
    "        csv_path=\"train.csv\",\n",
    "        image_root=\".\",\n",
    "        model_name=\"openai/clip-vit-base-patch32\",\n",
    "        batch_size=8,\n",
    "        epochs=5,\n",
    "        lr=1e-5,\n",
    "        margin=0.3,\n",
    "        device='mps'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9a120-f335-4a48-a3e7-4a9e2c573b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab7388-75fe-48ce-a85f-df8e30de3aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
