{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7785d5-2b44-4e0a-9f4f-5b2322ddc147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18840adf3e344d9c86888593a414efe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SPIQA_testA_Images.zip:   0%|          | 0.00/121M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-A/SPIQA_testA_Images.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"google/spiqa\", filename=\"test-A/SPIQA_testA.json\", repo_type=\"dataset\", local_dir='.')\n",
    "hf_hub_download(repo_id=\"google/spiqa\", filename=\"test-A/SPIQA_testA_Images.zip\", repo_type=\"dataset\", local_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fce25fc-30db-46f4-9510-04cbb07a1ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>question</th>\n",
       "      <th>reference_figure</th>\n",
       "      <th>all_figures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1611.04684v1</td>\n",
       "      <td>What are the main differences between the educ...</td>\n",
       "      <td>1611.04684v1-Table1-1.png</td>\n",
       "      <td>1611.04684v1-Table3-1.png,1611.04684v1-Table2-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1611.04684v1</td>\n",
       "      <td>Which model performs the best for response sel...</td>\n",
       "      <td>1611.04684v1-Table4-1.png</td>\n",
       "      <td>1611.04684v1-Table3-1.png,1611.04684v1-Table2-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1611.04684v1</td>\n",
       "      <td>Which model performs best on the Ubuntu datase...</td>\n",
       "      <td>1611.04684v1-Table5-1.png</td>\n",
       "      <td>1611.04684v1-Table3-1.png,1611.04684v1-Table2-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1611.04684v1</td>\n",
       "      <td>What is the role of the knowledge gates in the...</td>\n",
       "      <td>1611.04684v1-Figure1-1.png</td>\n",
       "      <td>1611.04684v1-Table3-1.png,1611.04684v1-Table2-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1611.04684v1</td>\n",
       "      <td>How does the average number of answers per que...</td>\n",
       "      <td>1611.04684v1-Table2-1.png</td>\n",
       "      <td>1611.04684v1-Table3-1.png,1611.04684v1-Table2-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper_id                                           question  \\\n",
       "0  1611.04684v1  What are the main differences between the educ...   \n",
       "1  1611.04684v1  Which model performs the best for response sel...   \n",
       "2  1611.04684v1  Which model performs best on the Ubuntu datase...   \n",
       "3  1611.04684v1  What is the role of the knowledge gates in the...   \n",
       "4  1611.04684v1  How does the average number of answers per que...   \n",
       "\n",
       "             reference_figure  \\\n",
       "0   1611.04684v1-Table1-1.png   \n",
       "1   1611.04684v1-Table4-1.png   \n",
       "2   1611.04684v1-Table5-1.png   \n",
       "3  1611.04684v1-Figure1-1.png   \n",
       "4   1611.04684v1-Table2-1.png   \n",
       "\n",
       "                                         all_figures  \n",
       "0  1611.04684v1-Table3-1.png,1611.04684v1-Table2-...  \n",
       "1  1611.04684v1-Table3-1.png,1611.04684v1-Table2-...  \n",
       "2  1611.04684v1-Table3-1.png,1611.04684v1-Table2-...  \n",
       "3  1611.04684v1-Table3-1.png,1611.04684v1-Table2-...  \n",
       "4  1611.04684v1-Table3-1.png,1611.04684v1-Table2-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('test-A/SPIQA_testA.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 2. Extract rows with just the three fields\n",
    "rows = []\n",
    "for paper in data.values():\n",
    "    pid      = paper.get('paper_id', '')\n",
    "    all_figs = ','.join(paper.get('all_figures', {}))\n",
    "    for qa in paper.get('qa', []):\n",
    "        rows.append({\n",
    "            'paper_id':        pid,\n",
    "            'question':        qa.get('question', ''),\n",
    "            'reference_figure':qa.get('reference', ''),\n",
    "            'all_figures':     all_figs\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'paper_id','question','reference_figure','all_figures'\n",
    "])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31816318-9738-4ea6-8dd4-d8065574396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df26dccc-5cb5-44a9-862c-da3d4fb33b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model(model_dir: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Load your fine-tuned CLIP model and processor from disk.\n",
    "    \"\"\"\n",
    "    processor = CLIPProcessor.from_pretrained(model_dir)\n",
    "    model     = CLIPModel.from_pretrained(model_dir).to(device)\n",
    "    model.eval()\n",
    "    return processor, model\n",
    "\n",
    "def compute_image_embeddings(image_paths, processor, model, device):\n",
    "    \"\"\"\n",
    "    Given a list of image file paths, load and preprocess them in a batch,\n",
    "    run through CLIP, and return normalized image embeddings + their filenames.\n",
    "    \"\"\"\n",
    "    images = [Image.open(p).convert(\"RGB\") for p in image_paths]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feats = model.get_image_features(**inputs)\n",
    "    # L2-normalize\n",
    "    img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n",
    "    return img_feats, [os.path.basename(p) for p in image_paths]\n",
    "\n",
    "def evaluate_retrieval(df: pd.DataFrame, image_root: str, processor, model, device):\n",
    "    top1 = top3 = top5 = total = 0\n",
    "\n",
    "    # Precompute image embeddings per paper_id (unchanged)\n",
    "    paper_embeddings = {}\n",
    "    for paper_id, group in df.groupby(\"paper_id\"):\n",
    "        figs = group[\"all_figures\"].iloc[0].split(\",\")\n",
    "        paths = [os.path.join(image_root, paper_id, fig) for fig in figs]\n",
    "        feats, names = compute_image_embeddings(paths, processor, model, device)\n",
    "        paper_embeddings[paper_id] = {\"feats\": feats, \"names\": names}\n",
    "\n",
    "    # Loop over every question\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        q_text   = row[\"question\"]\n",
    "        ref      = row[\"reference_figure\"]\n",
    "        paper_id = row[\"paper_id\"]\n",
    "\n",
    "        # —– FIXED: add truncation and max_length here —–\n",
    "        text_inputs = processor(\n",
    "            text=[q_text],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=processor.tokenizer.model_max_length\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            txt_feats = model.get_text_features(**text_inputs)\n",
    "        txt_feats = txt_feats / txt_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        img_feats = paper_embeddings[paper_id][\"feats\"]\n",
    "        img_names = paper_embeddings[paper_id][\"names\"]\n",
    "\n",
    "        sims = (txt_feats @ img_feats.T).squeeze(0).cpu().numpy()\n",
    "        ranked_idxs  = np.argsort(-sims)\n",
    "        ranked_names = [img_names[i] for i in ranked_idxs]\n",
    "\n",
    "        total += 1\n",
    "        if ref in ranked_names[:1]: top1 += 1\n",
    "        if ref in ranked_names[:3]: top3 += 1\n",
    "        if ref in ranked_names[:5]: top5 += 1\n",
    "\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Top-1 Acc: {top1/total:.4f}\")\n",
    "    print(f\"Top-3 Acc: {top3/total:.4f}\")\n",
    "    print(f\"Top-5 Acc: {top5/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65bbe908-ea4d-401e-9016-494bb85192e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_ROOT = \"test-A/SPIQA_testA_Images\"\n",
    "MODEL_DIR  = \"clip_finetuned\"\n",
    "df[\"all_figures\"]= df[\"all_figures\"].str.strip()\n",
    "df[\"reference_figure\"]  = df[\"reference_figure\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b7c154-3c0d-4c26-9433-e9fed8146e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE     = torch.device(\"mps\")\n",
    "processor, model = load_clip_model(MODEL_DIR, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd4943b-8f35-44cf-b7ca-5dfd7814b39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 666/666 [00:08<00:00, 74.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 666\n",
      "Top-1 Acc: 0.2943\n",
      "Top-3 Acc: 0.5661\n",
      "Top-5 Acc: 0.7162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_retrieval(df, IMAGE_ROOT, processor, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef934c9f-7526-4172-a529-048b42eb5728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1901.00056v2-Figure2-1.png', '1901.00056v2-Figure1-1.png', '1901.00056v2-Figure3-1.png', '1901.00056v2-Table5-1.png', '1901.00056v2-Table1-1.png', '1901.00056v2-Table3-1.png', '1901.00056v2-Table4-1.png', '1901.00056v2-Table6-1.png', '1901.00056v2-Table2-1.png']\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "for path in os.listdir(\"test-A/SPIQA_testA_Images\"):\n",
    "    images = os.listdir(os.path.join(\"test-A/SPIQA_testA_Images\", path))\n",
    "    print(images)\n",
    "    break\n",
    "    sizes.append(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5963fcb-3d71-4e3c-832f-2dea10430cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9af23028-c9a6-4daa-896c-278b9398c069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80dbb8f6-5cef-4a34-bcce-a84f574e42fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69544b-58bc-4c57-8e05-d41a044ade22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
